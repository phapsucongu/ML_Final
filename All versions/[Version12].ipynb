{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3709b2f4",
   "metadata": {
    "papermill": {
     "duration": 0.004094,
     "end_time": "2024-12-18T16:31:38.480018",
     "exception": false,
     "start_time": "2024-12-18T16:31:38.475924",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Based\n",
    "- https://www.kaggle.com/code/honganzhu/cmi-piu-competition?scriptVersionId=201912528 Version44 LB0.492\n",
    "\n",
    " If you find this notebook useful, please upvote this and the based one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e50ddf2",
   "metadata": {
    "papermill": {
     "duration": 0.002977,
     "end_time": "2024-12-18T16:31:38.486366",
     "exception": false,
     "start_time": "2024-12-18T16:31:38.483389",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Description of Imported Libraries\n",
    "\n",
    "- **NumPy (`np`)**: Used for efficient numerical operations, including linear algebra and array manipulation.\n",
    "- **Pandas (`pd`)**: Provides data structures like DataFrames for handling structured data, essential for data preprocessing.\n",
    "- **Polars (`pl`)**: A faster alternative to pandas for DataFrame operations, particularly useful for large datasets.\n",
    "- **Matplotlib & Seaborn (`plt`, `sns`)**: Visualization libraries. Matplotlib is used for basic plots, while Seaborn builds on it to create more advanced statistical visualizations.\n",
    "- **LightGBM, XGBoost, CatBoost**: Machine learning libraries used for gradient boosting, which is efficient for both regression and classification tasks.\n",
    "- **Colorama**: Enhances console output with colored text, making it easier to highlight important results or warnings.\n",
    "- **SciPy (`minimize`)**: Provides optimization routines, such as adjusting thresholds to maximize performance metrics like kappa scores.\n",
    "- **OS**: Used for file path manipulations and system-related functions.\n",
    "- **Scikit-learn (`sklearn`)**: A powerful machine learning library, providing utilities for cross-validation, metrics, and model cloning.\n",
    "- **YDF**: A specialized library for machine learning tasks, likely including decision forests.\n",
    "- **ThreadPoolExecutor & TQDM**: Tools for parallelizing tasks and displaying progress bars for long-running loops, improving efficiency and usability.\n",
    "- **Warnings**: Filters out unwanted warnings to keep the output clean, useful when dealing with noisy outputs from multiple libraries.\n",
    "- **IPython display (`clear_output`)**: A utility for clearing the Jupyter notebook output, often used to avoid clutter in long-running scripts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee2572f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:31:38.494497Z",
     "iopub.status.busy": "2024-12-18T16:31:38.494099Z",
     "iopub.status.idle": "2024-12-18T16:32:20.043458Z",
     "shell.execute_reply": "2024-12-18T16:32:20.042488Z"
    },
    "papermill": {
     "duration": 41.556073,
     "end_time": "2024-12-18T16:32:20.045768",
     "exception": false,
     "start_time": "2024-12-18T16:31:38.489695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip -q install /kaggle/input/pytorchtabnet/pytorch_tabnet-4.1.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e9c46d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:32:20.053988Z",
     "iopub.status.busy": "2024-12-18T16:32:20.053701Z",
     "iopub.status.idle": "2024-12-18T16:32:25.874331Z",
     "shell.execute_reply": "2024-12-18T16:32:25.873618Z"
    },
    "papermill": {
     "duration": 5.827025,
     "end_time": "2024-12-18T16:32:25.876373",
     "exception": false,
     "start_time": "2024-12-18T16:32:20.049348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a0ffdab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:32:25.885235Z",
     "iopub.status.busy": "2024-12-18T16:32:25.884368Z",
     "iopub.status.idle": "2024-12-18T16:32:42.136785Z",
     "shell.execute_reply": "2024-12-18T16:32:42.135873Z"
    },
    "papermill": {
     "duration": 16.258852,
     "end_time": "2024-12-18T16:32:42.138915",
     "exception": false,
     "start_time": "2024-12-18T16:32:25.880063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.optimize import minimize\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from colorama import Fore, Style\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import lightgbm as lgb\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e35c88b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:32:42.147223Z",
     "iopub.status.busy": "2024-12-18T16:32:42.146627Z",
     "iopub.status.idle": "2024-12-18T16:32:42.150467Z",
     "shell.execute_reply": "2024-12-18T16:32:42.149717Z"
    },
    "papermill": {
     "duration": 0.009502,
     "end_time": "2024-12-18T16:32:42.152028",
     "exception": false,
     "start_time": "2024-12-18T16:32:42.142526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1b0959",
   "metadata": {
    "papermill": {
     "duration": 0.003011,
     "end_time": "2024-12-18T16:32:42.159246",
     "exception": false,
     "start_time": "2024-12-18T16:32:42.156235",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering\n",
    "\n",
    "- **Feature Selection**: The dataset contains features related to physical characteristics (e.g., BMI, Height, Weight), behavioral aspects (e.g., internet usage), and fitness data (e.g., endurance time). \n",
    "- **Categorical Feature Encoding**: Categorical features are mapped to numerical values using custom mappings for each unique category within the dataset. This ensures compatibility with machine learning algorithms that require numerical input.\n",
    "- **Time Series Aggregation**: Time series statistics (e.g., mean, standard deviation) from the actigraphy data are computed and merged into the main dataset to create additional features for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7784941",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:32:42.166727Z",
     "iopub.status.busy": "2024-12-18T16:32:42.166496Z",
     "iopub.status.idle": "2024-12-18T16:32:42.180059Z",
     "shell.execute_reply": "2024-12-18T16:32:42.179372Z"
    },
    "papermill": {
     "duration": 0.019117,
     "end_time": "2024-12-18T16:32:42.181504",
     "exception": false,
     "start_time": "2024-12-18T16:32:42.162387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_file(filename, dirname):\n",
    "    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n",
    "    df.drop('step', axis=1, inplace=True)\n",
    "    return df.describe().values.reshape(-1), filename.split('=')[1]\n",
    "\n",
    "def load_time_series(dirname) -> pd.DataFrame:\n",
    "    ids = os.listdir(dirname)\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n",
    "    \n",
    "    stats, indexes = zip(*results)\n",
    "    \n",
    "    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n",
    "    df['id'] = indexes\n",
    "    return df\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim*3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*3, encoding_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*2, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, input_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim*2, input_dim*3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim*3, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def perform_autoencoder(df, encoding_dim=50, epochs=50, batch_size=32):\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df)\n",
    "    \n",
    "    data_tensor = torch.FloatTensor(df_scaled)\n",
    "    \n",
    "    input_dim = data_tensor.shape[1]\n",
    "    autoencoder = AutoEncoder(input_dim, encoding_dim)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(data_tensor), batch_size):\n",
    "            batch = data_tensor[i : i + batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = autoencoder(batch)\n",
    "            loss = criterion(reconstructed, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}]')\n",
    "                 \n",
    "    with torch.no_grad():\n",
    "        encoded_data = autoencoder.encoder(data_tensor).numpy()\n",
    "        \n",
    "    df_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i + 1}' for i in range(encoded_data.shape[1])])\n",
    "    \n",
    "    return df_encoded\n",
    "\n",
    "def feature_engineering(df):\n",
    "    season_cols = [col for col in df.columns if 'Season' in col]\n",
    "    df = df.drop(season_cols, axis=1) \n",
    "    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n",
    "    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n",
    "    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n",
    "    df['BFP_BMI'] = df['BIA-BIA_Fat'] / df['BIA-BIA_BMI']\n",
    "    df['FFMI_BFP'] = df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat']\n",
    "    df['FMI_BFP'] = df['BIA-BIA_FMI'] / df['BIA-BIA_Fat']\n",
    "    df['LST_TBW'] = df['BIA-BIA_LST'] / df['BIA-BIA_TBW']\n",
    "    df['BFP_BMR'] = df['BIA-BIA_Fat'] * df['BIA-BIA_BMR']\n",
    "    df['BFP_DEE'] = df['BIA-BIA_Fat'] * df['BIA-BIA_DEE']\n",
    "    df['BMR_Weight'] = df['BIA-BIA_BMR'] / df['Physical-Weight']\n",
    "    df['DEE_Weight'] = df['BIA-BIA_DEE'] / df['Physical-Weight']\n",
    "    df['SMM_Height'] = df['BIA-BIA_SMM'] / df['Physical-Height']\n",
    "    df['Muscle_to_Fat'] = df['BIA-BIA_SMM'] / df['BIA-BIA_FMI']\n",
    "    df['Hydration_Status'] = df['BIA-BIA_TBW'] / df['Physical-Weight']\n",
    "    df['ICW_TBW'] = df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d473e55b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:32:42.188981Z",
     "iopub.status.busy": "2024-12-18T16:32:42.188753Z",
     "iopub.status.idle": "2024-12-18T16:34:13.869594Z",
     "shell.execute_reply": "2024-12-18T16:34:13.868617Z"
    },
    "papermill": {
     "duration": 91.687172,
     "end_time": "2024-12-18T16:34:13.871904",
     "exception": false,
     "start_time": "2024-12-18T16:32:42.184732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 996/996 [01:12<00:00, 13.66it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 11.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 1.6832]\n",
      "Epoch [20/100], Loss: 1.6832]\n",
      "Epoch [30/100], Loss: 1.5466]\n",
      "Epoch [40/100], Loss: 1.4584]\n",
      "Epoch [50/100], Loss: 1.4166]\n",
      "Epoch [60/100], Loss: 1.4058]\n",
      "Epoch [70/100], Loss: 1.4043]\n",
      "Epoch [80/100], Loss: 1.4002]\n",
      "Epoch [90/100], Loss: 1.4001]\n",
      "Epoch [100/100], Loss: 1.4007]\n",
      "Epoch [10/100], Loss: 0.9944]\n",
      "Epoch [20/100], Loss: 0.7161]\n",
      "Epoch [30/100], Loss: 0.4287]\n",
      "Epoch [40/100], Loss: 0.4271]\n",
      "Epoch [50/100], Loss: 0.4271]\n",
      "Epoch [60/100], Loss: 0.4271]\n",
      "Epoch [70/100], Loss: 0.4271]\n",
      "Epoch [80/100], Loss: 0.4271]\n",
      "Epoch [90/100], Loss: 0.4271]\n",
      "Epoch [100/100], Loss: 0.4271]\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
    "sample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "\n",
    "train_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
    "test_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n",
    "\n",
    "df_train = train_ts.drop('id', axis=1)\n",
    "df_test = test_ts.drop('id', axis=1)\n",
    "\n",
    "train_ts_encoded = perform_autoencoder(df_train, encoding_dim=60, epochs=100, batch_size=32)\n",
    "test_ts_encoded = perform_autoencoder(df_test, encoding_dim=60, epochs=100, batch_size=32)\n",
    "\n",
    "time_series_cols = train_ts_encoded.columns.tolist()\n",
    "train_ts_encoded[\"id\"]=train_ts[\"id\"]\n",
    "test_ts_encoded['id']=test_ts[\"id\"]\n",
    "\n",
    "train = pd.merge(train, train_ts_encoded, how=\"left\", on='id')\n",
    "test = pd.merge(test, test_ts_encoded, how=\"left\", on='id')\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "numeric_cols = train.select_dtypes(include=['float64', 'int64']).columns\n",
    "imputed_data = imputer.fit_transform(train[numeric_cols])\n",
    "train_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\n",
    "train_imputed['sii'] = train_imputed['sii'].round().astype(int)\n",
    "for col in train.columns:\n",
    "    if col not in numeric_cols:\n",
    "        train_imputed[col] = train[col]\n",
    "        \n",
    "train = train_imputed\n",
    "\n",
    "train = feature_engineering(train)\n",
    "train = train.dropna(thresh=10, axis=0)\n",
    "test = feature_engineering(test)\n",
    "#test_submit = test\n",
    "\n",
    "train = train.drop('id', axis=1)\n",
    "test  = test .drop('id', axis=1)   \n",
    "\n",
    "\n",
    "featuresCols = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-CGAS_Score', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'sii', 'BMI_Age','Internet_Hours_Age','BMI_Internet_Hours',\n",
    "                'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight',\n",
    "                'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW']\n",
    "\n",
    "featuresCols += time_series_cols\n",
    "\n",
    "train = train[featuresCols]\n",
    "train = train.dropna(subset='sii')\n",
    "\n",
    "featuresCols = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-CGAS_Score', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'BMI_Age','Internet_Hours_Age','BMI_Internet_Hours',\n",
    "                'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight',\n",
    "                'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW']\n",
    "\n",
    "featuresCols += time_series_cols\n",
    "test = test[featuresCols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "437b76f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:34:13.917488Z",
     "iopub.status.busy": "2024-12-18T16:34:13.914476Z",
     "iopub.status.idle": "2024-12-18T16:34:13.927286Z",
     "shell.execute_reply": "2024-12-18T16:34:13.926479Z"
    },
    "papermill": {
     "duration": 0.037717,
     "end_time": "2024-12-18T16:34:13.929038",
     "exception": false,
     "start_time": "2024-12-18T16:34:13.891321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if np.any(np.isinf(train)):\n",
    "    train = train.replace([np.inf, -np.inf], np.nan)\n",
    "if np.any(np.isinf(test)):\n",
    "    test = test.replace([np.inf, -np.inf], np.nan)\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "164f4744",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:34:13.965427Z",
     "iopub.status.busy": "2024-12-18T16:34:13.965143Z",
     "iopub.status.idle": "2024-12-18T16:34:13.972548Z",
     "shell.execute_reply": "2024-12-18T16:34:13.971783Z"
    },
    "papermill": {
     "duration": 0.027155,
     "end_time": "2024-12-18T16:34:13.974338",
     "exception": false,
     "start_time": "2024-12-18T16:34:13.947183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_part1 = train[train['Enc_1'].isnull()]\n",
    "train_part2 = train[train['Enc_1'].notnull()]\n",
    "train_part1 = train_part1.loc[:, ~train_part1.columns.str.contains('Enc')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78faa6a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:34:14.011621Z",
     "iopub.status.busy": "2024-12-18T16:34:14.011264Z",
     "iopub.status.idle": "2024-12-18T16:34:14.021656Z",
     "shell.execute_reply": "2024-12-18T16:34:14.020681Z"
    },
    "papermill": {
     "duration": 0.031556,
     "end_time": "2024-12-18T16:34:14.024201",
     "exception": false,
     "start_time": "2024-12-18T16:34:13.992645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_part1 = test[test['Enc_1'].isnull()]\n",
    "test_part2 = test[train['Enc_1'].notnull()]\n",
    "test_part1 = test_part1.loc[:, ~test_part1.columns.str.contains('Enc')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bd51b41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:34:14.074447Z",
     "iopub.status.busy": "2024-12-18T16:34:14.074124Z",
     "iopub.status.idle": "2024-12-18T16:34:14.083731Z",
     "shell.execute_reply": "2024-12-18T16:34:14.082929Z"
    },
    "papermill": {
     "duration": 0.034496,
     "end_time": "2024-12-18T16:34:14.085761",
     "exception": false,
     "start_time": "2024-12-18T16:34:14.051265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## code model here\n",
    "CatBoost_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'iterations': 200,\n",
    "    'random_seed': SEED,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 10,  # Increase this value\n",
    "    'task_type': 'GPU'\n",
    "}\n",
    "\n",
    "cat_boost_model = CatBoostRegressor(**CatBoost_Params)\n",
    "\n",
    "## model 2\n",
    "# Model parameters for LightGBM\n",
    "Params = {\n",
    "    'learning_rate': 0.046,\n",
    "    'max_depth': 12,\n",
    "    'num_leaves': 478,\n",
    "    'min_data_in_leaf': 13,\n",
    "    'feature_fraction': 0.893,\n",
    "    'bagging_fraction': 0.784,\n",
    "    'bagging_freq': 4,\n",
    "    'lambda_l1': 10,  # Increased from 6.59\n",
    "    'lambda_l2': 0.01,  # Increased from 2.68e-06\n",
    "    'device': 'gpu' # dòng này có hoặc không\n",
    "}\n",
    "\n",
    "\n",
    "# XGBoost parameters\n",
    "XGB_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 1,  # Increased from 0.1\n",
    "    'reg_lambda': 5,  # Increased from 1\n",
    "    'random_state': SEED,\n",
    "    'tree_method': 'gpu_hist',\n",
    "}\n",
    "\n",
    "XGB_Model = XGBRegressor(**XGB_Params)\n",
    "\n",
    "### LGBMRegressor\n",
    "Params7 = {'learning_rate': 0.1, 'max_depth': 10, 'num_leaves': 100, 'min_data_in_leaf': 10,\n",
    "           'feature_fraction': 0.5, 'bagging_fraction': 0.5, 'bagging_freq': 2, \n",
    "           'lambda_l1': 1, 'lambda_l2': 1e-04} # CV : 0.4094 | LB : 0.471\n",
    "\n",
    "Light = lgb.LGBMRegressor(**Params7, verbose=-1, n_estimators=200, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5decf5fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:34:14.138233Z",
     "iopub.status.busy": "2024-12-18T16:34:14.137880Z",
     "iopub.status.idle": "2024-12-18T16:34:14.152457Z",
     "shell.execute_reply": "2024-12-18T16:34:14.151595Z"
    },
    "papermill": {
     "duration": 0.034679,
     "end_time": "2024-12-18T16:34:14.154111",
     "exception": false,
     "start_time": "2024-12-18T16:34:14.119432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def TrainML(train, sample, model_class, test_data):\n",
    "    X = train.drop(['sii'], axis=1)\n",
    "    y = train['sii']\n",
    "\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model = clone(model_class)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        test_preds[:, fold] = model.predict(test_data)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    \n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': sample['id'],\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "208a856c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:34:14.189916Z",
     "iopub.status.busy": "2024-12-18T16:34:14.189669Z",
     "iopub.status.idle": "2024-12-18T16:34:36.098542Z",
     "shell.execute_reply": "2024-12-18T16:34:36.097280Z"
    },
    "papermill": {
     "duration": 21.929226,
     "end_time": "2024-12-18T16:34:36.100697",
     "exception": false,
     "start_time": "2024-12-18T16:34:14.171471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|██████████| 5/5 [00:04<00:00,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train QWK --> 0.9153\n",
      "Mean Validation QWK ---> 0.5033\n",
      "----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.543\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sub_Light_1 = TrainML(train, sample, Light, test)\n",
    "sub_cat_boost_model_1 = TrainML(train, sample, cat_boost_model, test)\n",
    "sub_XGB_Model_1 = TrainML(train, sample,XGB_Model, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c901d536",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:34:36.140034Z",
     "iopub.status.busy": "2024-12-18T16:34:36.139371Z",
     "iopub.status.idle": "2024-12-18T16:34:36.160564Z",
     "shell.execute_reply": "2024-12-18T16:34:36.159863Z"
    },
    "papermill": {
     "duration": 0.041621,
     "end_time": "2024-12-18T16:34:36.162194",
     "exception": false,
     "start_time": "2024-12-18T16:34:36.120573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub1 = sub_Light_1.sort_values(by='id').reset_index(drop=True)\n",
    "sub2 = sub_cat_boost_model_1.sort_values(by='id').reset_index(drop=True)\n",
    "sub3 = sub_XGB_Model_1.sort_values(by='id').reset_index(drop=True)\n",
    "\n",
    "combined = pd.DataFrame({\n",
    "    'id': sub1['id'],\n",
    "    'sii_1': sub1['sii'],\n",
    "    'sii_2': sub2['sii'],\n",
    "    'sii_3': sub3['sii']\n",
    "})\n",
    "\n",
    "def majority_vote(row):\n",
    "    return row.mode()[0]\n",
    "\n",
    "combined['final_sii'] = combined[['sii_1', 'sii_2', 'sii_3']].apply(majority_vote, axis=1)\n",
    "\n",
    "final_submission = combined[['id', 'final_sii']].rename(columns={'final_sii': 'sii'})\n",
    "\n",
    "final_submission.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cee09cd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-18T16:34:36.198751Z",
     "iopub.status.busy": "2024-12-18T16:34:36.198502Z",
     "iopub.status.idle": "2024-12-18T16:34:36.210785Z",
     "shell.execute_reply": "2024-12-18T16:34:36.209780Z"
    },
    "papermill": {
     "duration": 0.03254,
     "end_time": "2024-12-18T16:34:36.212672",
     "exception": false,
     "start_time": "2024-12-18T16:34:36.180132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008ff9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fd460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00105258</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00115b9f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016bb22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001f3379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0038ba98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0068a485</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0069fbed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0083e397</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0087dd65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00abe655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00ae59c9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00af6387</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00bd4359</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00c0cd71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00d56d4b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00d9913d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00e6167c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00ebc35d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sii\n",
       "0   00008ff9    1\n",
       "1   000fd460    0\n",
       "2   00105258    1\n",
       "3   00115b9f    0\n",
       "4   0016bb22    1\n",
       "5   001f3379    1\n",
       "6   0038ba98    1\n",
       "7   0068a485    0\n",
       "8   0069fbed    1\n",
       "9   0083e397    1\n",
       "10  0087dd65    1\n",
       "11  00abe655    1\n",
       "12  00ae59c9    1\n",
       "13  00af6387    0\n",
       "14  00bd4359    1\n",
       "15  00c0cd71    1\n",
       "16  00d56d4b    0\n",
       "17  00d9913d    1\n",
       "18  00e6167c    0\n",
       "19  00ebc35d    1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_submission"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9643020,
     "sourceId": 81933,
     "sourceType": "competition"
    },
    {
     "datasetId": 921302,
     "sourceId": 7453542,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30776,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 183.914178,
   "end_time": "2024-12-18T16:34:39.651911",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-18T16:31:35.737733",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
